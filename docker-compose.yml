services:
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraping-pipeline
    volumes:
      # Montar directorio principal para archivos Excel
      - .:/app/output
      # Montar directorios específicos para resultados
      - ./data:/app/data
      - ./backup:/app/backup
      - ./logs:/app/logs
      # Montar archivos de configuración (opcional)
      - ./firebase-credentials.json:/app/firebase-credentials.json:ro
    environment:
      # Variables de entorno opcionales
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - TZ=America/Bogota
    # Recursos del contenedor - aumentados para evitar que se mate
    deploy:
      resources:
        limits:
          memory: 5G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    # Red personalizada (opcional)
    networks:
      - scraping-network
    # Reiniciar política
    restart: unless-stopped
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  scraping-network:
    driver: bridge

# Volúmenes persistentes (opcional)
volumes:
  scraping-data:
    driver: local
